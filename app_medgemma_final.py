import streamlit as st
from transformers import AutoProcessor, AutoModelForImageTextToText
from huggingface_hub import login
from PIL import Image
import torch

# ‚Äî 1. Page configuration & branding ‚Äî
st.set_page_config(page_title="WeCare MedGemma AI", layout="wide")
with st.sidebar:
    st.image(
        "https://wecarehealth.co.nz/wp-content/uploads/2023/07/wecare_logo.svg",
        width=180
    )
    st.markdown("**Created by Dr. John Ko**")
    st.markdown("---")

# ‚Äî 2. Title & instructions ‚Äî
st.title("üß† WeCare MedGemma 4B ‚Äì Medical Image Assistant")
st.markdown(
    "Upload a dermatoscope, X-ray, or fundus image, type your question, "
    "and get an AI-powered analysis via Google‚Äôs MedGemma 4B."
)

# ‚Äî 3. Authenticate to Hugging Face ‚Äî
try:
    login(token=st.secrets["HF_TOKEN"])
except Exception as e:
    st.error(f"Authentication failed: {e}")
    st.stop()

# ‚Äî 4. Image uploader & prompt input ‚Äî
uploaded = st.file_uploader("üì§ Upload medical image", type=["jpg","jpeg","png"])
prompt   = st.text_input(
    "üí¨ Ask a question about the image:",
    "What condition is shown in this image?"
)

if not (uploaded and prompt):
    st.info("Please upload an image and enter a question above.")
    st.stop()

# ‚Äî 5. Display image ‚Äî
try:
    image = Image.open(uploaded)
    image.verify()
    uploaded.seek(0)
    image = Image.open(uploaded).convert("RGB")
except Exception:
    st.error("Invalid image file. Please upload a valid image.")
    st.stop()

st.image(image, caption="Uploaded Image", use_column_width=True)

# ‚Äî 6. Lazy-load & cache processor + model ‚Äî
@st.cache_resource(show_spinner=False)
def load_medgemma():
    model_id = "google/medgemma-4b-it"
    # bfloat16 only on GPU; float32 on CPU
    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    processor = AutoProcessor.from_pretrained(
        model_id,
        trust_remote_code=True
    )
    model = AutoModelForImageTextToText.from_pretrained(
        model_id,
        trust_remote_code=True,
        device_map="auto",
        torch_dtype=dtype
    ).eval()
    return processor, model

processor, model = load_medgemma()

# ‚Äî 7. Build the ‚Äúchat‚Äù input & run inference :contentReference[oaicite:0]{index=0}
with st.spinner("üîç Running analysis..."):
    # Apply the chat template (adds <start_of_image>, tokenizes)
    inputs = processor.apply_chat_template(
        [
            {"role":"system", "content":[{"type":"text","text":"You are an expert medical AI."}]},
            {"role":"user",   "content":[{"type":"text","text":prompt}, {"type":"image","image":image}]}
        ],
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    ).to(model.device, dtype=model.dtype)

    # Generate and strip off the input prompt tokens
    with torch.inference_mode():
        generated = model.generate(**inputs, max_new_tokens=512)
    gen_tokens = generated[0, inputs["input_ids"].shape[-1]:]

    # Decode to plain text
    response = processor.decode(gen_tokens, skip_special_tokens=True)

# ‚Äî 8. Display the AI‚Äôs answer ‚Äî
st.markdown("### üß† AI Response")
st.success(response)
